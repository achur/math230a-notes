\documentclass[12pt]{article}
\usepackage{e-jc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{algorithmic}
\usepackage{fancyhdr}
\usepackage{hyperref}

\hypersetup {
  colorlinks=true,
  linkcolor=black,
  citecolor=blue,
  pdftitle={Math 230A Notes}
  pdfauthor={Churchill}
  pdfsubject={Notes from Math230A/Stat310A Probability Theory}
  pdfkeywords={Probability}{Notes}
}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\long\def\symbolfootnote[#1]#2{\begingroup%
\def\thefootnote{\fnsymbol{footnote}}\footnote[#1]{#2}\endgroup}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{corr}{Corollary}

\title{Math 230A / Stat 310A -- Probability Theory -- Notes}

\author{
Alex Churchill\\
\small \texttt{achur@stanford.edu}
}

\date{Autumn, 2011}

\begin{document}

\maketitle
\thispagestyle{empty} % ignore page number on first page
\tableofcontents


\newpage

\setcounter{page}{1} % set page number back to 1

\section{Course Information}
Prof. Persi Diaconis, Sequoia 131, 725-1965 (no email) \\
Office Hours: Wednesday 1:30 - 3:00
\\ \\
TA. Anirban, Sequoia 208 (anirbanb@stanford.edu) \\
Office Hours: Friday 10-12
\\ \\
TA. Sumit, Sequoia 237 (sumitm@stanford.edu) \\
Office Hours: Monday 2-4
\\ \\
Text: P. Billingsley, \underline{Probability and Measure} 3rd Ed. (On reserve at Math Library).
\\ \\
Grading: Homework (30\%), Midterm (30\%), Final (40\%)
\\ \\
Midterm: Thursday Nov. 3, in class; 5:30-8:00 pm; One $3 \times 5$ notecard allowed.
\\ \\
Final: Thursday, Dec. 15, 7-10 pm

\subsection{Homeworks}

{\bf HW WEEK 2: READ Sec 3, 4.  Do problems 3.2(a,b); 3.3(a,b,c,d); 3.11; 3.13; 3.16; 4.11}

{\bf HW WEEK 3: READ Sec 10, 11, 14.  Do problems 10.1, 10.2, 14.5, 14.8.}
Problem: Let $F_1, F_2$ be distribution functions on $\mathbb{R}$.  Define $H_l(x,y) = (F_1(x) + F_2(y) - 1)_+$ where $(x)_+ = x$ if $x \ge 0$ and $0$ otherwise.  Define $H_u(x,y) = \min(F_1(x), F_2(y))$.  (a) Prove that $H_l$, $H_u$ are bivariate distribution functions with margins $F_1(x) = H_l(x, \infty) = H_u(x, \infty)$ and $F_2(y) = H_l(\infty, y) = H_u(\infty, y)$. (b) Prove for all $H(x,y)$, $F_1, F_2$ as margins, $H_l(x,y) \le H(x,y) \le H_u(x,y)$ for $-\infty < x,y < \infty$.
\\ \\
{\bf HW WEEK 4: READ Sec 15, 16.  Do Problems 15.1, 15.2, 16.1, 16.7 + PROBLEM (see it down there in the notes)}


\section{Week 1}
This week covers material from sections 1 and 2 in the book.

\subsection{Introduction}
We start by posing a simple probability problem: how many people must be in a room for even odds that two people will have the same birthday?
\\ \\
I wasn't in class for this derivation, so I'm not sure exactly how it happened, but it is easy enough to estimate using the Poisson distribution's approximation of the binomial distribution (though note I'm fairly sure this wasn't the approximation used in class).
\\ \\
Recall that the Binomial Distribution determines the number of successes of $n$ experiments drawn with probability $p$.  In this case, given $N$ people in the room, the number of total birthday pairs is ${N \choose 2}$.  For each pair, the probability the two birthdays are on the same day is $1/365$.  Therefore, to apply the Poisson approximation, the expected number of birthday pairs is $\lambda = {N \choose 2}/365$, so we estimate the probability of $k$ successful trials to be $\frac{\lambda^k e^{-\lambda}}{k!}$.  Therefore, the probability there are no successful trials is approximately $e^{-{N \choose 2}/365}$, which turns out to be slightly less than $0.5$ for $N = 23$.
\\ \\
{\it Variations:}\\ \\
Let the probability a person is born on day $i$ be $\theta_i$ (in the usual case, $\theta_i = 1/365$) where $\sum \theta_i = 1$.
\\ \\
How many people for even odds of $j$ matching birthdays in $N$ days? \\
Answer: In general, $k = \{ N^{j-1} ln \frac{1}{1-p} \}^{1/j}$.\\
Cash \$10 to whomever proves this.
\\ \\
\subsection{Coin Tossing}
We introduce a model for fair coin tossing:
\\ \\
Let $\Sigma$ be the interval $(0, 1]$.  For $0 < a \le b \le 1$, define $P([a,b]) = b-a$.
\\ \\
If $I_1, ..., I_k$ are disjoint intervals, define $P(\cup_{i=1}^k I_k) = \sum_{i=1}^k |I_k|$.
\\ \\
Write $\omega \in [0,1]$ as binary: $\omega = \sum_{i=1}^\infty \frac{d_i(\omega)}{2^i}$.
\\ \\
$d_i: (0,1] \rightarrow \{0, 1\}$ where $d_i$ is constructed by breaking $(0, 1]$ into $i$ equally-sized intervals and assigning $d_i$ to 0, 1, 0, 1, ... among those intervals (not a formal definition!).
\\ \\
Then $P(\{ \omega: d_i(\omega) = 1\}) = 1/2$, so we say $P\{d_i = 1\} = 1/2$.
\\ \\
$P(d_1 = d_2 = 1) = 1/4$, and more generally, $P(d_1 = e_1, ..., d_k = e_k) = 1/2^k$ for $e_i \in \{0, 1\}$.

\subsection{Strong Law (Coin Tossing)}
\begin{lemma}
(Markov's Inequality): Given $f:(0, 1] \rightarrow [0, \infty)$, $P\{\omega: f(\omega) \ge a\} \le \frac{ \int_0^1 f(\omega) d \omega }{a}$.
\end{lemma}
\begin{proof}
Note that we break the integral into two pieces: $\int_{\{ \omega : f(\omega) \ge a \}} f(\omega) d\omega +\int_{\{ \omega : f(\omega) \ge a \}} f(\omega) d\omega$ at which point the proof simply notices that the second integral must be positive and the first is at least $a P\{ \omega : f(\omega) \ge a \}$
\end{proof}

\begin{theorem}
(Weak Law of Large Numbers for Coin Tossing): For all $\epsilon > 0$, $P \{ | \frac{1}{n} \sum d_i - 1/2 | > \epsilon \} \rightarrow 0$ as $n \rightarrow \infty$.
\end{theorem}
\begin{proof}
Note it is easier to work with $r_i(\omega) = 2 \cdot d_i(\omega) - 1$; it is enough to show $P\{ | \frac{1}{n} \sum_{i=1}^n r_i | > \epsilon \} \rightarrow 0$.
\\ \\
Evaluate $\int_0^1 r_i(\omega) d\omega = 0$ and $\int_0^1 r_i(\omega) r_j(\omega) d\omega = 1$ if $i = j$ and $0$ otherwise.
\\ \\
Hence, $\int_0^1 (\sum_{i=1}^n r_i(\omega) )^2 d\omega = n$.
\\ \\
Finally, note $P \{ | \frac{1}{n} \sum_{i=1}^n r_i | > \epsilon \} = P \{ | \sum_{i=1}^n r_i |^2 > n^2 \cdot \epsilon^2 \} \le \frac{1}{n \epsilon^2} \rightarrow 0$ by Markov's inequality.
\end{proof}

Now, we would like to say
$$\lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n d_i(\omega) = \frac{1}{2}$$
but we can't:
$$ \omega = 0.00111100000000111111111111111100...$$

So instead...

\begin{theorem}
(Strong Law of Large Numbers for Coin Tossing): $\lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n d_i(\omega) = \frac{1}{2}$ except for $\omega$ in a negligible set (later: true for $\omega$ almost everywhere).
\end{theorem}
\begin{proof}
Let $r_i = 2d_i - 1$.  Show $\frac{1}{n} \sum_{i=1}^n r_i \rightarrow 0$.
\\ \\
Last time, $P\{ | \frac{s_n}{n} | > \epsilon \} = P \{ s_n^4 \ge (\epsilon n)^4 \}$.
\\ \\
By Markov's inequality,
$$\le \frac{ \int_0^1 |s_n \omega|^4 d \omega }{\epsilon^4 n^4} \le \frac{3}{\epsilon^4 n^2}$$
We show $\int |s_n|^4 \le 3n^2$.
\\ \\
Choose $\epsilon_n \to 0$ so $\sum_{n=1}^\infty \frac{1}{n^2 \epsilon_n^4} < \infty$.
e.g. let $\epsilon_n = \frac{1}{n^{1/5}}$, let $B = \{ \omega: lim \textrm{ } exists \}$, $A_n = \{ \omega: | \frac{s_n}{n} | \ge \epsilon_n \}$.
Then if $\omega \in \cap_{n=m}^\infty A_n^C$, $\omega$ in $B$:
$$\cap_{n=m}^\infty A_n^C \subset B \textrm{ on } B^C \subset \cup_{n=m}^\infty A_n$$
$A_n = \cup_{k=1}^{k(n)} I_{nk}$ so $|A_n| \le \frac{1}{n^2 \epsilon_n^4}$\\
$b^C \subset \cup_{n=1, k=1}^\infty I_{nk}$ so $\sum_{k=1, n=m}^\infty |I_{n,k}| < \epsilon$.
\\ \\
\end{proof}
Remarks: Borel proved this first, and was studying the number theory problem:\\ \\
For $\omega \in (0, 1]$, the proportion of the first $n$ binary digits tends to $1/2$; same for all bases simultaneously.
\\ \\
Difference between weak and strong laws: weak law says gets close to 1/2, strong law says for almost everywhere, gets close and stays there.
\\ \\
Notice that $B = \{ \omega : \lim \frac{s_n}{n} = 0 \} = \cap_{k=1}^\infty \cup_{m=1}^\infty \cap_{n=m}^\infty \{ \omega: |\frac{s_n}{n} | < \frac{1}{k} \}$ is a complicated set.
\\ \\
\subsection{Fields and $\sigma$-Algebras}

\begin{defn}
Let $\Omega$ be any set, and $\mathcal{F}_0$ a collection of subsets is a {\bf Field} if:

\begin{enumerate}
\item  $\emptyset, \Omega \in \mathcal{F}_0$
\item  $A \in \mathcal{F}_0 \Rightarrow A^C \in \mathcal{F}_0$
\item  $A_1, ..., A_n \in \mathcal{F}_0 \Rightarrow \cup_{i=1}^n A_i \in \mathcal{F}_0$
\end{enumerate}

\end{defn}

\begin{defn}
Let $\Omega$ be any set, and $\mathcal{F}_0$ a collection of subsets is a {\bf $\sigma$-Algebra} if:

\begin{enumerate}
\item  $\emptyset, \Omega \in \mathcal{F}_0$
\item  $A \in \mathcal{F}_0 \Rightarrow A^C \in \mathcal{F}_0$
\item  $A_1, ... \in \mathcal{F}_0 \Rightarrow \cup_{i=1}^\infty A_i \in \mathcal{F}_0$
\end{enumerate}

\end{defn}

That is, a $\sigma$-Field is a Field closed under countable unions.
\\ \\
Example: The Borel sets are the minimal $\sigma$-algebra containing all open intervals $I \in [-\infty, \infty]$.  Note that there is no easy description of Borel sets and constructing them requires transfinite operations.  But it doesn't matter because Borel sets will just work the way we want them to anyway.
\\ \\

\begin{defn}
$(\Sigma, \mathcal{F})$ be a (set, $\sigma$-algebra).  A function $P$ is a {\bf probability} if

\begin{enumerate}
\item $P(\emptyset) = 0$, $P(\Omega) = 1$
\item $P(A) = 1 - P(A^C)$ for all $A \in \mathcal{F}$
\item $A_i \in \mathcal{F}$ for all $1 \le i < \infty$, $P(\cup_1^\infty A_i) = \sum_1^\infty P(A_i)$ for all $A_i$ disjoint.
\end{enumerate}

\end{defn}

In other words, $P$ is just a measure that takes on value $1$ for $\Omega$.
\\ \\
Task: Say $\mathcal{F}_0$ field of subsets.  Given $P$ on $\mathcal{F}_0$, want to extend $P$ to $\sigma(\mathcal{F}_0)$.  (We follow the Greeks)
\\ \\
Define: For al $A \in \Sigma$ $P^*(A) = \inf \sum_{i=1}^\infty P(A_i)$ where $A \subset \cup_{i=1}^\infty A_i$.
\\ \\
In this case, $P^*$ is an outer measure (and for our interval definition, is in fact the Lebesgue outer measure).
\\ \\
Some obvious facts: (1) $P^*(\emptyset) = 0$.  (2) Similarly, $P^*(\Omega) = 1$.  (3) If $A \subset B$, then $P^*(A) \le P^*(B)$.  (4) Given $\{A_i\}$ any sets in $\Omega$, $P^*(\cup A_i) \le \sum_{i=1}^\infty P^*(A_i)$ (subadditivity).
\\ \\
Proof of (4): \\
Fix $\epsilon > 0$, choose $B_{ik} \in \mathcal{F}_0$ : $\cup B_{ik} \supset A_i$ and $\sum_{i=1}^\infty P(B_{ik}) \le P^*(A_i) + \epsilon/2^i$.
\\ \\
Then $\cup_1^\infty A_i \subset \cup_{i.k} B_{ik}$ so $P^*(\cup A_i) \le \sum_1^\infty (P^*(A_i) + \epsilon / 2^i) = \sum_1^\infty P^*(A_i) + \epsilon$.  QED.
\\ \\
What have we learned since the Greeks?  We can't assign a length to all subsets of (0,1].  We need a clever way of approximating (or maybe just a way to do some rigorous math) so outer measures $(P^*)$ work there.
\\ \\
Idea: (Caratheodory): \\
Given $\Omega$, $\mathcal{F}_0$, $P$ on $\mathcal{F}_0$, $A$ probability, define $P^*$ as above.
\\ \\
Let $M = \{ A : \forall E, P^*(E) = P^*(A \cap E) + P^*(A^C \cap E) \}$.  (Collection of all measurable sets).
\\ \\
We show: (1) $M$ is a $\sigma$-algebra containing $\mathcal{F}_0$, (2) $P^*$ is countably additive on $M$ (in other words, $P^*$ is a measure on $M$), (3) $P^*$ agrees with $P$ for sets in $\mathcal{F}_0$, (4) $P^*$ is unique on $M$ given (1-3).
\\ \\
Note that $M = \{ A : \forall E, P^*(E) \ge P^*(A \cap E) + P^*(A^C \cap E) \}$ by subadditivity.
\\ \\
Proof that $M$ is a field: Clearly contains $\emptyset$ and $\Omega$.  Because of symmetry, if $A \in M$, $A^C$ in $M$.  Now, say $A$, $B$ in $M$.  Then $P^*(E) = P^*(B \cap E) + p^*(B^C \cap E) = P^*(A \cap B \cap E) + P^*(A^C \cap B \cap E) + P^*(A \cap B^C \cap E) + P^*(A^C \cap B^C \cap E) \ge P^*(A \cap B \cap E) + P^*((A^C \cap B \cap E) \cup (A \cap B^C \cap E) \cup (A^C \cap B^C \cap E)) = P^*((A \cap B) \cap E) + P^*((A \cap B)^C \cap E)$.


\section{Week 2}
This week covers material from sections 3 and 4 in the book.

\subsection{$\sigma$-algebras}
Given $\Omega$ and $\mathcal{F}_0$ a field of subsets of $\Omega$.  $P$ is a probability given on $\mathcal{F}_0$.  We want to extend to $P^*$, the outer measure generated by $P$; for any set $A \subset \Omega$
$$P^*(A) = \inf \sum_{i=1}^\infty P(A_i)$$
where $A_i \in \mathcal{F}_0$ such that $A \subset \cup A_i$.  (Note this is just a normalized Lebesgue outer measure).
\\ \\
Easy to show that 
\begin{enumerate}
\item $P^*(\emptyset) = 0$
\item $P^*(\Omega) = 1$
\item $A \subset B \Rightarrow P^*(A) \le P^*(B)$
\item $P^*$ is countably subadditive.
\end{enumerate}

Let $\mathcal{M} = \{ A \subset \Omega : \forall E \subset \Omega, P^*(E) = P^*(E \cap A) + P^*(E \cap A^C) \}$.  These are measurable sets under $P$.
\\ \\
Just as a heads-up (we'll prove this later), the Caratheadory theorm says:
\begin{enumerate}
\item $\mathcal{M}$ is a $\sigma$-algebra containing $\mathcal{F}_0$
\item $P^*$ is a probability on $\mathcal{M}$
\item $P^*(A) = P(A)$ for $A \in \mathcal{F}_0$
\item $P^*$ is the unique such extension
\end{enumerate}

Last time, we proved $\mathcal{M}$ is a field.  We want to prove $\mathcal{M}$ is a $\sigma$-field.
\\ \\
{\bf Fact:} If $\{ A_i \}_{n=1}^\infty \in \mathcal{M}, E \subset \Omega$, $A_i$ disjoint,
$$P^*(E \cap (\cup_{i=1}^\infty A_i)) = \sum_{i=1}^\infty P^*(E \cap A_i)$$
{\bf Proof} by induction.  If $n = 1$ we're OK.  If $n = 2$, we have:
$$P^*(E \cap (A_1 \cup A_2)) = P^*(E \cap (A_1 \cup A_2) \cap A_1) + P^*(E \cap (A_1 \cup A_2) \cap A_2)$$
$$ = P^*(E \cap A_1) + P^*(E \cap A_2)$$
Same for all finite $n$.  In general, $P^*(E \cap (\cup_{i=1}^
\infty A_i)) \ge P^*(E \cap (\cup_{i=1}^n A_i)) = \sum_{i=1}^n P^*(E \cap A_i)$.  Taking the limit as $n \to \infty$, we get $P^*(E \cap (\cup_{i=1}^\infty A_i)) \ge \sum_{i=1}^\infty P^*(E \cap A_i)$.  The other direction follows by subadditivity.
\\ \\
{\bf Fact:} $\mathcal{M}$ is a $\sigma$-algebra and $P^*$ is a probability on $\mathcal{M}$.
\\ \\
{\bf Proof:}
Given $A_n \in \mathcal{M}$ for $1 \le n < \infty$ where $A_i'$ is defined by $A_1' = A_1$, $A_2' = A_2 \cap A_1^C$, $A_3' = A_3 \cap A_1^C \cap A_2^C$, etc.  Therefore, $A_i'$ disjoint for all $i$, but $\cup_{i=1}^\infty A_i = \cup_{i=1}^\infty A_i'$.
\\ \\
So without loss of generality, we can say all $A_i$ are disjoint.
\\ \\
Want $P^*(E) \ge P^*(E \cap (\cup A_i)) + P^*(E \cap (\cup A_i)^C$.
\\ \\
Set $F_n = \cup_{i=1}^n A_i$.  Then 
$$P^*(E) = P^*(E \cap F_n) + P^*(E \cap F_n^C)$$
$$ \ge \sum_{i=1}^n  P^*(E \cap A_i) + P^*(E \cap (\cup_{i=1}^\infty A_i)^C)$$
so as $n \to \infty$
$$P^*(E) \ge \sum_{i=1}^\infty P^*(E \cap A_i)$$
$$ \ge P^*(E \cap (\cup_{i=1}^\infty A_i)) + P^*(E \cap (\cup_{i=1}^\infty A_i)^C)$$
The reverse inequality follows directly from $A \in \mathcal{M}$ and $P^*$ countably additive on $\mathcal{M}$.
\\ \\
Note: $\mathcal{F}_0 \subset \mathcal{M}$, pick $A \in \mathcal{F}_0$ and $E \subset \Omega$.
\\ \\
From the definition of $P^*(E)$, for all $\epsilon > 0$, there exist $A_1, A_2, ...$ with $A_n \in \mathcal{F}_0$ where $E \subset \cup_{i=1}^\infty A_i$ and $\sum_{i=1}^\infty P(A_i) \le P^*(E) + \epsilon$.
\\ \\
Let $B_n = A_n \cap A$, $C_n = A_n \cap A^C$.  $E \cap A \subset \cup_{i=1}^n B_n$, $E \cap A^C \subset \cup C_n$.

$$P^*(E \cap A) + P^*(E \cap A^C) \le \sum P(B_n) + \sum P(C_n) = \sum P(A_n) \le P^*(E) + \epsilon$$
Letting $\epsilon \to 0$ gives us what we want.
\\ \\
Further, we note $P^*(A) = P(A)$ if $A \in \mathcal{F}_0$.  To show this, we know $P^*(A) \le P(A)$.  If $A \subset \cup A_i$, where $A_i \in \mathcal{F}_0$, $P(A) \le \sum P(A \cap A_i) \le \sum P(A_i)$ so $P^*(A) \ge P(A)$.
\\ \\
For Uniqueness, we need to define a $\Pi$ system.

\begin{defn}
A class of subsets $\mathcal{P}$ is a $\Pi$-system if it is closed under finite intersection.
\end{defn}

\begin{defn}
A set $\mathcal{L}$ of subsets is called a $\lambda$-system if
\begin{enumerate}
\item $\Omega \in \mathcal{L}$
\item $A \in \mathcal{L} \Rightarrow A^C \in \mathcal{L}$
\item $A_1, A_2, ... \in \mathcal{L}$, with all $A_i$ disjoint guarantees $\cup_{i=1}^\infty A_i \in \mathcal{L}$.
\end{enumerate}
\end{defn}

\begin{theorem}
(Dynkin's $\Pi$-theorem) If $\mathcal{P}$ is a $\Pi$-system and a $\mathcal{L}$ is a $\lambda$-system, $P \subset \mathcal{L}$, then $\sigma(\mathcal{P}) \subset \mathcal{L}$
\end{theorem}
\begin{proof}
If $A, B \in \mathcal{L}$, $A \subset B$, then $B \backslash A = B \cap A^C \in \mathcal{L}$.  Proof: $A \cup B^C \in \mathcal{L}$, so $B \cap A^C \in \mathcal{L}$.
\\ \\
Because the intersection of $\lambda$-systems is a $\lambda$-system, there is a smallest $\lambda$-system containing $\mathcal{P}$, call it $\mathcal{L}_0$.  We show that $\mathcal{L}_0$ is a $\Pi$-system.  Then we're done for $\mathcal{L}_0$, since it is a $\sigma$-algebra containing $\mathcal{P}$, so $\mathcal{L}$ is a $\sigma$-algebra containing $\mathcal{P}$.
\\ \\
Let $A \in \mathcal{L}_0$.  Let $\mathcal{L}_A = \{ E \subset \Sigma: E \cap A \in \mathcal{L}_0 \}$.  We claim $\mathcal{L}_A$ is a $\lambda$-system.  
\end{proof}

\begin{theorem}
Corrolary: If $u$ and $v$ are probabilities that agree on the $\Pi$-system $\mathcal{P}$, then they agree on $\sigma(\mathcal{P})$
\end{theorem}
\begin{proof}
Let $\mathcal{L}$ be all subsets of $A$ where $m(A) = v(A)$.  This is a $\lambda$-system, so $m(A) = v(A)$ for all $A \in \sigma(\mathcal{P})$.  If $B \in \mathcal{L}_A$, $A \cap B \in \mathcal{L}_0$, $A \cap (A \cap B)^C = A \cap B^C \in \mathcal{L}_0$ so $B^C \in A$.
\\ \\
If $\{B_i\}$ disjoint in $\mathcal{L}_A$ then $A \cap (\cup B_i) = \cup(A \cap B_i)$ so $\cup B_i \in \mathcal{L}_A$.
\\ \\
Say $A, B \in \mathcal{P}$ so $A \cap B \in \mathcal{P}$ so $A \in \mathcal{L}_B$.  But $\mathcal{L}_B$ is a $\lambda$-system, so $\mathcal{L}_0 \subset \mathcal{L}_B$; e.g. for every $A \in \mathcal{L}_0$, $B \in \mathcal{L}_A$, so $\mathcal{L}_0 \subset \mathcal{L}_A$ for all $A \in \mathcal{L}_0$.
\\ \\
So if $B, C \in \mathcal{L}_0$ then $C \in \mathcal{L}_A$; e.g. $C \cap A \in \mathcal{L}_0$.  So $\mathcal{L}_0$ is closed under finite intersections.  So $\mathcal{L}_0$ is a $\Pi$-system.
\end{proof}

Therefore, if $P$ is a probability on a field $\mathcal{F}_0 \le 2^\Omega$, then extension $P^*$ to $\sigma( \mathcal{F}_0)$ is unique.
\\ \\
Note we assumed $P$ was a probability on $\mathcal{F}_0$; that is we assume $A$, $A_1, A_2, ...\in \mathcal{F}_0$ with $A = \cup A_i$ and all $A_i$ disjoint then $P(A) = \sum P(A_i)$.  This check is performed in the book.

\subsection{Extensions}
YEAH THERE NEEDS TO BE SOME STUFF FILLED IN HERE... (Wk 2., Day 2).

\section{Week 3}
This week covers material from sections 10 - 12 and 14 in the book.

\subsection{$\infty$ measures}
\begin{defn}
$\Omega$ any set, $\mathcal{F}$ is a field of subsets of $\Omega$ ($\emptyset \in \mathcal{F}$, $\mathcal{F}$ closed under finite intersections and complementation).  $\mu: \mathcal{F} \rightarrow [0, \infty]$, $\mu(\emptyset) = 0$, $\mu(\cup_1^\infty A_i) = \sum \mu(A_i)$ is a measure on $\mathcal{F}$.  In other words, a measure is 0 on the emptyset, nonnegative, and countably additive.
\end{defn}

If $\mu(\Omega) < \infty$, it is the same as a probability.  If $\exists A_n \in \mathcal{F}, \Omega = \cup_{n=1}^\infty A_n, \mu(A_n) < \infty$ for all $n$, $\mu$ is $\sigma$-finite.
\\ \\
Example: $\Omega = \mathbb{N}$, $\mu(i) = 1$, $\mu$ is $\sigma$-finite.  Similarly, $\lambda$ on $\mathbb{R}$ is $\sigma$-finite (cover $[0,1]$, $[1,2]$, ...).  However, if $\Omega = [0,1]$ and $\mu(A) = $ the number of points in $A$, $\mu$ is not $\sigma$-finite.
\\ \\
Why do we want to talk about this?
\begin{enumerate}
\item Probability densities on $\mathbb{R}$; for example, $\frac{e^{-x^2/2}}{\sqrt{2 \pi}}$ with respect to length on $\mathbb{R}$.
\item In the $\sigma$-finite case, it is easy.
\end{enumerate}
Most arguments are ``same''.  For example, if $A_n \uparrow A$, $A_n, A \in \mathcal{F}$.  Then $\mu(A_n) \to \mu(A)$.  Proof.  $B_1 = A_1$, $B_n = A_n \backslash A_{n-1}$.  $\cup B_n = A$.  $\mu(A) = \sum \mu(B_i) = \lim_{n \to \infty} \sum_1^n \mu(B_i) = \lim_{n \to \infty} \mu(A_n)$.
\\ \\
But sometimes you need to watch it.  If $\mu$ is a probability, $A \subset B$, $\mu(B \backslash A) = \mu(B) - \mu(A)$ and if $A_n \downarrow A$ $\mu(A_n) \to \mu(A)$.  But on $\Omega = (-\infty, \infty)$, $A = (-\infty, 0]$, $B = (-\infty, 1]$, $\mu(B \backslash A) = 1$.  But $\mu(B) - \mu(A) = \infty - \infty$.
\\ \\
Uniqueness of extensions.  Given $\mathcal{P}$ a $\Pi$-system and $M_1, M_2$ measures on $\mathcal{P}$:
\begin{theorem}
If for some $\{B_i\}_{i=1}^\infty$, $B_i \in \mathcal{P}$ where $\mu_j(B_i) < \infty$ for $j = 1, 2$ and $\mu_1 = \mu_2$ on $\mathcal{P}$ then $M_1 = M_2$ on $\sigma(\mathcal{P})$.
\end{theorem}
\begin{proof}
Fix $B \in \mathcal{P}$ where $\mu_j(B) < \infty$ for $j=1,2$.  Let $\nu_j(F) = \mu_j(F \cap B)$.  These are finite measures, so by $\Pi-\lambda$ theorem, $\nu_1 = \nu_2$ on $\sigma(\mathcal{P})$.  Now, let $A_1 = B_1$, $A_2 = B_2 \backslash A_1$, $A_n = B_n \backslash (\cup_{i=1}^n A_i)$.  Then for all $F$, $\mu_1(F) = \mu_1(F \cap (\cup_i A_i)) = \sum_i \mu_1 (F \cap A_i) = \sum_i \mu_2 (F \cap A_i) = \mu_2(F \cap (\cup_i A_i)) = \mu_2(F)$.
\end{proof}
Note that if things are not $\sigma$-finite you can have two different extensions.  See the book.
\\ \\
\subsection{Back to Outer Measures}
\begin{defn}
$\mu^*$ is an outer measure on $\Omega$ if $\mu^* : 2^\Omega \rightarrow [0, \infty]$, $\mu^*(\emptyset) = 0$, and $\mu^*(\cup A_i) \le \sum \mu^*(A_i)$.  In other words, nonnegative, nontrivial, and countably subadditive.
\end{defn}

Example: $\Omega$ any set, $\mathcal{A}$ any collection of subsets, $\emptyset \in \mathcal{A}$.  $\rho : \mathcal{A} \rightarrow [0, \infty]$ any function.  Define $\mu_\rho^* (A) = \inf \sum_{i=1}^\infty \rho(A_n)$, $A \subset \cup_{i=1}^\infty A_i$, and $\mu_\rho^*(A) = \infty$ if no such cover exists.  Claim: $\mu^*$ is an outer measure.  $\emptyset$ follows trivially (it is covered by zero sets).  The measure is definitionally nonnegative.  It is also obviously countably subadditive.
\\ \\
Example: Hausdorff $\gamma$ measure on $\mathbb{R}^n$.  Let $\mathcal{A}$ be the collection all closed balls $B_n(x)$. $\rho(B)$ is the volume of the ball.  $\gamma$ is fixed in $(-\infty, \infty)$.  Read more in 2nd edition of billingsley.
\\ \\
As usual, given an outer measure $\mu^*$, define $\mathcal{M}(\mu^*) = \{ A \subset \omega: \forall E \subset \Omega, \mu^*(E) = \mu^*(E \cap A) + \mu^*(E \cap A^C) \}$.
\begin{theorem}
$\mathcal{M}(\mu^*)$ is a $\sigma$-algebra and $\mu^*$ is a measure on $\mathcal{M}$.
\end{theorem}
\begin{proof}
Sentence for sentence same proof as for probabilities.
\end{proof}
To work with $\infty$ measures, it is useful to know $\sigma$-rings, $\Omega$ a set.

\begin{defn}
A collection of subsets $\mathcal{A}$ is a $\sigma$-ring if $\emptyset \in \mathcal{A}$, $A,B \in \mathcal{A} \Rightarrow A \cap B \in \mathcal{A}$, $A, B \in \mathcal{A}$ and $A \le B$ then $\exists C_i, 1 \le i \le n$ disjoint in $\mathcal{A}$ such that $B \backslash A = \cup_{i=1}^n C_i$.
\end{defn}

Example: On $\mathbb{R}$ consider all $(a, b)$ with $-\infty \le a \le b \le \infty$ form a $\sigma$-ring.
\\ \\
\begin{theorem}
(Extension Theorem) Let $\mu$ be a function on a $\sigma$-ring of subsets $\mathcal{A}$ with $\mu(A) \in [0, \infty]$, $\mu(\emptyset) = 0$, $\mu$ finitely additive and $\mu$ countably subadditive: for all $A_i, \cup A_i \in \mathcal{A}$, $\mu(\cup A_i) \le \sum_1^\infty \mu(A_i)$.  Then $\mu$ extends to a measure on $\sigma(\mathcal{A})$ and if there exists $A_i$ contained in $\mathcal{A}$ such that $\Omega = \cup_1^\infty A_i$, $\mu(A_i) < \infty$ the extension is unique.
\end{theorem}
\begin{proof}
Define $\mu^*(A) = \inf \sum_{i=1}^\infty \mu(A_i)$ wher e$A \subset \cup_1^\infty A_i$ and $A_i \in \mathcal{A}$.  That is an outer measure and $\mu^*$ on $\mathcal{M}(\mu^*)$ is a $\sigma$-algebra that does the job.  We show (1) $\mathcal{A} \subset \mathcal{M}(\mu^*)$ and (2) $\mu^*(A) = \mu(A)$ for all $A \in \mathcal{A}$.
\\ \\
For (1), pick $A \in \mathcal{A}$.  We must show for all $E$, $\mu^*(E) \ge \mu^*(E \cap A) + \mu^*(E \cap A^C)$.  If $\mu^*(E) = \infty$ it is true.  Suppose $\mu^*(E) < \infty$.  Then for every $\epsilon$ there exists $A_i \in \mathcal{A}$ such that $\sum \mu(A_i) \le \mu^*(E) + \epsilon$.  Since $\mu^*$ is finite, $\mu^*(A_i)$ is finite for all $i$.  Set $B_n = A \cap A_n$. $B_n \in \mathcal{F}$.  Now $B_n \subset A_n$ so $A_n \backslash B_n = \cup_{i=1}^{m_n} C_{ni}$ where disjoint $C_{ni} \in \mathcal{A}$.  Then $A_n = B_n \cup (\cup_{i=1}^{m_n} C_{ni})$, $A \cap E \subset \cup B_n$.  Then $A^C \cap  E \subset \cup_n \cup_i C_{ni}$.  Now 
$$\mu^*(E \cap A) + \mu^*(E \cap A^C) \le \sum_{n=1}^\infty \mu(B_n) + \sum_{n=1}^\infty \sum_{i=1}^{m_n} \mu(C_{ni})$$
$$ = \sum_{n=1}^\infty \mu(B_n) + \mu(A_n - B_n)$$
$$ = \sum_n \mu(A_n) \le \mu^*(E) + \epsilon$$
For (2), if $A \subset \cup A_i$ where $A, A_i \in \mathcal{A}$, then $\mu(A) \le \sum_i \mu(A_i)$.  The other direction is free.
\end{proof}


\subsection{Distribution functions on $\mathbb{R}$}
Given probability $\mu$ we describe a {\bf distribution function} by $F(x) = \mu(-\infty, x]$.  We often define probability measures using distribution functions.  For instance, the ``Gauss Measure'' $F(x) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^x e^{-t^2/2} dt$.
\\ \\
Observe (1) $\lim_{x \to -\infty} F(x) = 0$, $\lim_{x \to \infty} F(x) = 1$ (normalization), and $y < x$ means $F(x) - F(y) = \mu(-\infty, x] - \mu(-\infty, y] = \mu(y, x] \ge 0$.  Therefore, (2) $F$ is monotone.
\\ \\
Further, (3) if $x_n \downarrow x$, $(-\infty, x_n] \downarrow (-\infty, x]$, so $F(x_n) \downarrow F(x)$ so $F(x)$ is right continuous.  Note it isn't left continuous.  Let $\mu(A) = 1$ if $0 \in A$ and $0$ otherwise.  Then $F(x) = 0$ for $x < 0$ and $F(x) = 1$ for $x \ge 0$.
\begin{theorem}
Conversely, if $F(x)$ satisfies (1. normalization), (2. monotonicity), and (3. right-continuity), then $\exists !$ probability measure on $(-\infty, \infty)$ with $F(x) = \mu(-\infty, x]$.
\end{theorem}

Note $\{(-\infty, x]: x \in \mathbb{R}\}$ is a $\Pi$-system.
\\ \\
Want to do this in higher dimensions.
\\ \\
Let $A_{x_1, x_2} = \{ (\eta_1, \eta_2) : \eta_i < x_i \}$.  Given $\mu$ on the Borel sets of $\mathbb{R}^2$, define $H(x_1, x_2) = \mu(A_{x_1, x_2})$.  $H$ is monotone, right continuous, but need a bit more.  Note $\mu(A) = \mu(A_x) - \mu(A_y) - \mu(A_w) + \mu(A_z)$
\begin{verbatim}
 -----------------
 .      |        |
 . A_w  |  A_x   |
 .      |        |
 -------|--------|
 .      |        |
 . A_z  |  A_y   |
 . ...  |  ...   |

\end{verbatim}
In $\mathbb{R}_d$, let $A = \{(x_1, ..., x_d) : a_i \le x_i \le b_i\}$.  If $\underline{v}$ is a vertex, $sign(\underline{v}) = -1$ if $\underline{v}$ has an odd number of $a_i$'s, and $1$ otherwise.
\\ \\
If $H(x_1, ..., x_d), \delta_A H = \sum_{\underline{v}} sign(\underline{v}) H(\underline{v})$.  $H$ satisfies $H(x_1, ..., x_d) = \mu(A_{x_1, ..., x_d})$ for a unique probability $\mu$ $\Leftrightarrow$ $\lim_{\underline{x} \to \infty} H(\underline{x}) = 0$, $\lim_{\underline{x} \to \infty} H(\underline{x}) = 1$ (min coord goes to $-\infty$ or max coord goes to $\infty$), $H$ is right continuous and for all rectangles, $\prod(x_i, B_i] = \delta_A H \ge 0$.
\\ \\
So now we have a HW problem from this week: \\Let $F_1(x), F_2(x)$ be distribution functions on $\mathbb{R}$.  $H(x,y)$ with $H(x, \infty) = F_1(x)$ and $H(\infty, y) = F_2(y)$ is called a bivariate distribution function with margins $F_1$ and $F_2$.
\\ \\
Problem a: Consider $H_L(x, y) = (F_1(x) + F_2(y) - 1)$ ($H$-lower) and $H_U = \min(F_1(x), F_2(y))$ ($H$-upper).  Check that these are distribution functions with margins $F_1$ and $F_2$.
\\ \\
Problem b: For every $H$ with margins $F_1, F_2$, show $H_L(x,y) \le H(x,y) \le H_U(x,y)$.
\\ \\
Remark: Once we know what correlation is, $H_L$ is the most negatively correlated D.F. with margins $F_1$, $F_2$ and $H_U$ is the most positively correlated.
\\ \\
\subsection{Measurable Functions and Random Variables}
Let $(\Omega, \mathcal{F})$, $(\Omega', \mathcal{F}')$ be measure spaces.

\begin{defn}
A function $T: \Omega \rightarrow \Omega'$ is {\bf measurable} if\\
For all $A' \in \mathcal{F}'$, $T^{-1}(A') \in \mathcal{F}$
\end{defn}

Proposition: Suppose $\mathcal{F}' = \sigma(\mathcal{A})$.  (a) Then $T$ is measureable iff $T^{-1}(A') \in \mathcal{F}$, $A' \in \mathcal{A}'$.
\\ \\
(b) If $T_1 : (\Sigma_1, \mathcal{F}_1) \rightarrow (\Sigma_2, \mathcal{F}_2)$ and $T_2 : (\Sigma_2, \mathcal{F}_2) \rightarrow (\Sigma_3, \mathcal{F}_3)$ are measurable, then $T_2 \circ T_1 : (\Sigma_1, \mathcal{F}_1) \rightarrow (\Sigma_3, \mathcal{F}_3)$ is measurable.
\\ \\
Both follow directly.
\\ \\
\begin{defn}
A {\bf random variable} is a measurable function $T : (\Omega, \mathcal{F}) \rightarrow (\mathbb{R}, \mathcal{B}(\mathbb{R}))$ where $\mathcal{B}(\mathbb{R})$ is the class of the Borel sets.
\end{defn}
\begin{defn}
A {\bf random vector} is a measurable function $(\Omega, \mathcal{F}) \rightarrow (\mathbb{R}^n, \mathcal{B}(\mathbb{R}^n))$ where $T(\underline{x}) = (T_1(x_1), ..., T_n(x_n))$.
\end{defn}

\begin{lemma}
$T$ is a random variable $\Leftrightarrow$ all $T_i$ are random variables.
\end{lemma}
\begin{proof}
If each $T_i$ is a measure, then $T^{-1}(A_{\underline{x}}) = \cap_{i=1}^n T_i^{-1}(x_i)$.  Note this is enough by proposition (a), $A' = \{A_x\}$.
\\ \\
If $T$ is a random vector, then $T_i^{-1} (-\infty, x] = \cup_{n=1}^\infty T^{-1} \{ \underline{y} : y_i \le x_i \textrm{ and } y_j \le n, j \neq i\}$, so each is measurable.
\end{proof}

\begin{lemma}
If $T: \mathbb{R}^k \rightarrow \mathbb{R}$ is continuous, then $T$ is measurable.
\end{lemma}
\begin{proof}
Preimage of an open set is open (and closed set is closed) iff $T$ is continuous.  In $\mathbb{R}$, $T$ continuous $\Leftrightarrow T^{-1}(-\infty, x]$ is closed and closed sets are Borel measurable.
\end{proof}

\begin{corr}
If $X$ and $Y$ are random variables on $\Omega$, then $X+Y$, $X \cdot Y$, $\min(X, Y)$, $\max(X,Y)$ are random variables.
\end{corr}
\begin{proof}
$\Omega \rightarrow \mathbb{R}^2 \rightarrow \mathbb{R}$: composite functions will be measurable, since $f(x,y) = x+y$ is measurable.  Same for all others.
\end{proof}

\subsubsection{New measures from old:}
Say $\mu$ is a measure on $(\Omega, \mathcal{F})$ and $T: (\Omega, \mathcal{F}) \rightarrow (\Omega', \mathcal{F}')$ is measurable.  Define push forward of measurable $T$
$$\mu(T^{-1} (A'))  = \mu(T^{-1}(A')) = \mu \{ \omega : T(\omega) \in A'\}$$
This is a measure.
\\ \\
We use this to construct measures.  Example: let $O_n$ be the orthogonal group: that is, the set of all $n times n$ matrices $M$ such that $MM^T = I$.  Want to know what it means to ``pick a matrix at random''.  Suppose we know how to pick from the normal distribution.  Let $X_{ij}$ be independent picks from $\frac{e^{-x^2/2}}{\sqrt{2 \pi}}$ (good ol' bell-shaped curve).  As math: $\Omega = \mathbb{R}^{n^2}$, define $\mathcal{P}$ as $\mathcal{P}(A_{x_{11}, ..., x_{nn}}) = \int_{-\infty}^{x_{11}} ... \int_{-\infty}^{x_{nn}} \frac{e^{-\sum x_{ij}^2}}{(\sqrt{2 \pi})^n} d x_{11} ... x_{nn}$.  Map $T: \mathbb{R}^{n^2} \rightarrow O_n$ (Gram-Schmidt).  Then $P^{T^{-1}}$ is Harr measure.
\\ \\
This stuff is all sort of near section 15 of the book.  

\section{Week 4}
This week covers material from sections ~14,15 in the book.

\subsection{Lebesuge Interval}
Let $(\Omega, \mathcal{F}, \mu)$ be a measure space.  For measureable $f$, define
$$\int f d \mu = \int_\Omega f(\omega) \mu(d \omega)$$
Strategy to define:
\begin{enumerate}
\item Define it for $f \in SF_+$ (simple functions)
\item Define for $f \in m \mathcal{F}_)$
\item Extend to $f \in m \mathcal{F}$
\end{enumerate}
Read Wikipedia for all of this.

\begin{theorem}
(1) $f \in SF_+$, $f = \sum_{i=1}^m f_i I_{A_i} \Rightarrow \int f d \mu = \sum_{i=1}^m f_i \mu(A_i)$ \\ \\
(2) For all $\omega$, $0 \le f(\omega) \le g(\omega) \Rightarrow 0 \le \int f d \mu \le \int g d \mu$.
\\ \\
(3) For all $\omega$, $0 \le f_n(\omega) \le f (\omega)$, $f_n(\omega) \uparrow f(\omega) \Rightarrow \int f_n d \omega \uparrow \int f  d \mu$.
\\ \\
(4) $\alpha, \beta \ge 0$, $\int(\alpha f + \beta g) d \mu = \alpha \int f d \mu + \beta \int g d \mu$
\end{theorem}
\begin{proof}
(1) Note that $\ge$ is obvious (sup over all simple functions...).  Now, let $\{B_1, ..., B_n\}$ be a partition with $\beta_i = \inf_{\omega \in B_i} f (\omega)$.  Let $\{C_1, ..., C_k\}$ be the partition such that for each $A_i$, $B_j$, there are some $C_l, C_{l+1}, ..., C_{l+h}$ with $\cup C_l = A_i$, $\cup C_h = A_j$ (in other words, partition by both $A$ and $B$).  Then consider $\sum_{i=1}^n \beta_i \mu(B_i) \le \sum_{i=1}^h \gamma_i \mu(C_i) \le \sum_{i=1}^k f_i A_i$.
\\ \\
(2) We know $0 \le \int f d \mu$.  Now, let $f(\omega) \le g(\omega)$.  Then $\int f d \mu = \sup \sum_{i=1}^m \inf \omega \in A_i f (\omega) \mu(A_i) \le \sum \sum_{i=1}^m \inf \omega \in A_i g(\omega) \mu(A_i) = \int g d \mu$.
\\ \\
(3) $\int f_n d \mu$ non-decreasing, $\int f_n d \mu \le \int f d \mu$.  Hence, $\lim_{n \to \infty} \int f_n d \mu \le \int f d \mu$.
\\ \\
Note it is sufficient to prove for all partitions, $\lim_{n \to \infty} \int f_n d \mu \ge \sum_{i=1}^m \mu(A_i) f_i$ i.e. for all $\epsilon$, there is a large enough $n$ such that
$$\int f_n d \mu \ge \sum_{i=1}^m (f_i - \epsilon) \mu(A_i)$$
Let $A_{i, n} = \{ \omega \in A_i : f_n(\omega) \ge x_i - \epsilon \}$.  $A_{0, n} = \Omega \backslash \cup_{i=1}^m A_{i, n}$.  $\int f_n d \mu \ge \int - \epsilon \sum_{i=1}^m \mu(A_i)$ for $i = 1, ..., m$.
\\ \\
$\int f_n d \mu \ge \sum_{i=1}^m \inf_{\omega \in A_{i, n}} f_n(\omega) \mu(A_{i, n}) \ge \sum_{i=1}^m (x_i - \epsilon) \mu(A_{i,n})$ so $A_{i, n} \uparrow A_i$.
\\ \\
Note this assumes $\mu(A_i) < \infty$.  If $\mu(A_1), ..., \mu(A_{m_0}) < \infty, \mu(A_{m_0 + 1}), ..., \mu(A_m) = \infty$.  $S < \infty, \inf_{\omega \in A_i} f(\omega) = 0$ for all $i \in \{m_0 + 1, ..., m\}$.
\\ \\
(4) For $\int \alpha f d \mu = \sup_{\{A_i\}} \sum_{i=1}^m \mu(A_i) \inf_{\omega \in A_i} [ \alpha f(\omega) ] = \alpha \sup \sum \mu(A_i) \inf f(\omega) = \alpha \int f d \mu$.
\\ \\
Let $f, g$ be simple.  Obvious for $f, g$.  Now, all functions are limits from below of simple functions, so the whole thing becomes obvious.

\end{proof}

Review: Let $(\Omega, \mathcal{F},\mu)$ be a measure space, and let $f : \Omega \rightarrow [0, \infty]$ be a measurable function.  Then we define $\int f d \mu = \int_\Omega f (\omega) \mu(d \omega) = \sup_{\{A_n\}} \sum_{i=1}^N \inf_{\omega \in A_i} f (\omega_i) \mu(A_i)$, where $\Omega = \cup A_i$.
\\ \\
Properties:
\begin{enumerate}
\item $0 \le f \le g \Rightarrow \int f d \mu \le \int g d \mu$
\item Integral is linear
\item (Monotone Convergence Theorem) If $f_n, f \ge 0$ and $f_n(\omega) \uparrow f(\omega)$, then $\lim \int f_n d \mu = \int \lim f_n d \mu = \int f d \mu$.
\item If $f(\Omega) = \sum_{i=1}^N x_i \delta_{B_i}$ (step function), then $\int f d \mu = \sum_{i=1}^n x_i \mu(B_i)$.
\end{enumerate}

That is, monotonicity, linearity, MCT, and step functions.
\\ \\
If $f : \Omega \to \mathbb{R}$, write $f^+(\omega) = \max(f(\omega), 0)$ and $f_-(\omega) = \max(-f(\omega), 0)$.  Say $\int f d \mu = \int f^+ d \mu - \int f_- d \mu$.
\\ \\
\begin{theorem}
(Fatou's Lemma).  On $(\Omega, \mathcal{F}, P)$, let $f_n \ge 0$ be any measurable functions.  Then $\int \lim f_n d \mu \le \lim \int f_n d \mu$.
\end{theorem}
\begin{proof}
Set $g_n  = \inf_{h \ge n} f_h \uparrow g = \lim f_n$.  So $\lim \int g_n d \mu = \int \lim f_n d \mu$ (since $g_n$ are monotone).  But $g_n \le f_n$ so $\int g_n \le \int f_n$.  Taking the limit of both sides gives $\lim \int g_n \le \lim \int f_n$.  QED.
\end{proof}
Remarks:  This works for any $f_n \ge 0$.
\\ \\
Example: Enumerate the rationals in $[0,1]$ as $\Omega_1, \Omega_2, ...$.  Define $f(x) = \sum_{i=1}^\infty \frac{1}{i^2 \sqrt{|\Omega_i - x|}}$.  Claim: $f(x) < \infty$ (almost sure).
\\ \\
Proof: Let $f_n(x) = \sum_{i=1}^n \frac{1}{n^2} \frac{1}{\sqrt{|\Omega_i = x|}} \uparrow f(x)$.  Then $\int_0^1 f(x) dx \le \lim \sum \frac{1}{i^2} \int_0^1 \frac{dx}{\sqrt{|\Omega_i - x|} } \le \sum_{i=1}^\infty \frac{c}{i^2}$.
\\ \\
\$10 problem:
Find a single $x$ such that $f(x) < \infty$.
\\ \\
\begin{theorem}
(Dominated Convergence Theorem) On $(\Omega, \mathcal{F}, \mu)$, $f_n, f, g$ be measurable functions, $f_n(\omega) \rightarrow f(\omega)$ almost everywhere (i.e. almost sure), $|f_n| \le g$ and $\int g d \mu < \infty$, then $f_n, f$ are integrable and $\lim_{n \to \infty} \int f_n d \mu = \int f d \mu$.
\end{theorem}
\begin{proof}
By hypothesis, $f_n^+ + f_n^- \le g$, $f_* = \lim \inf f_n$ and $f^* = \lim \sup f_n$ are both $\le g$.  Then $g + f^*$ and $g - f_* \ge 0$.  Therefore, $\int g d \mu + \int f_* d \mu = \int \lim \inf (g + f_n) d \mu \le \int g d  \mu + \lim \inf \int f_n d \mu$ by Fatou.
\\ \\
For all $x_n$ check $\lim \inf -x_n = - \lim \sup x_n$.  Then $\int g d \mu - \int f^* d \mu = \int \lim \inf (g - f_n) d \mu \le \int g d \mu - \lim \sup \int f_n d \mu$ by Fatou.
\\ \\
$\int \lim \inf f_n d \mu \le \lim \inf \int f_n d \mu \le \lim \sup \int f_n d \mu \le \int \lim \sup f_n d \mu$.  But because $f_n$ converges almost surely, everything is equal.
\end{proof}

A little probability (and a Homework Problem):
\\ \\
In English, Let $x_n$ for $1 \le n \le \infty$ be an independent exponential $(P(X_i > x) = e^{-x})$, $M_n = -\max_{1 \le i \le n} X_i$.  Find limit behavior of $M_n$.
\\ \\
In Math: Let $\Omega = \mathbb{R}^n$, $\mathcal{F}$ borel.  Let $G(x_1, ..., x_n) = \prod_{i=1}^\infty (1 - e^{-x_i})_+$.  This is a distribution function which given $G(box) = \prod \int_{a_i}^{b_i} e^{-x} dx \ge 0$.  Let $P$ be associated probability.  $X_i(\omega_1, ..., \omega_n)$.  $P(M_n \le x) = P(X_i \le x \textrm{ for all } i) = (1 - e^{-x})^n = e^{n \log(1 - e^{-x})}$.  So, for $x$ large, $\log(1 - e^{-x}) ~ e^{-x}$, set $x = \log n + C$.  $P(M_n \le x) ~ e^{-e^{-c}}$ (that is, the extreme value distribution).
\\ \\
HW Problem: \\
(a): $x \ge 0$, $\frac{x}{1 + x^2} e^{-x^2/2} \le \int_x^\infty e^{-t^2/2} dt \le \frac{e^{-x^2/2}}{x}$.  \\ \\
(b): Let $x_i$ be i.i.d. in $\mathcal{N}(0,1)$ (normal about $(0,1)$).  Let $y_i = \lfloor x_i \rfloor$.  Let $M_n = \max(y_i)$.  Show there exist integers $a_n, p_n \in (0,1)$, $P(M_n = a_n) ~ p_n$, $P(M_n = a_n - 1) ~ (1 - p_n)$. \\ \\
(c):  $\lim \inf p_n \neq \lim \sup p_n$.

\section{Week 5}
This week covers material from sections 15 and 16 in the book.

\subsection{Independence and Expectation}

\subsection{Borel-Cantelli}


\section{Week 6}
This week covers material from sections 18 - 20 in the book.

\subsection{Product Measures}

\subsection{Fubini's Theorem}

\subsection{Kernels}


\section{Week 7}
This week covers material from sections 21 and 22 in the book.

\subsection{Moments}

\subsection{Uniform Integrability}

\subsection{Strong Law}


\section{Week 8}
This week covers material from section 25in the book.

\subsection{Weak Convergeance}


\section{Week 9}

\subsection{Poisson}

\subsection{Stein's Method}


\section{Week 10}

\subsection{Central Limit Theorem}


% This is the Bibliography
%%%%%%%%%%%%%%%
\newpage
\begin{thebibliography}{99}

\end{thebibliography}

\end{document}
